{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Tasting Data (UCI, 2015)\n",
    "This dataset was downloaded from the UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCTION**  \n",
    "Three wine-tasting experts (herein refered to as TASTERS) tasted ~ 6500 samples of Portuguese \"Vinho Verde\" wine, where ~30% and ~ 70% of the data are the red and white variants respectively. Each taster rated the QUALITY of samples on scale of 0 - 10 (0 = bad, 10 = great) and the final recorded quality rating is the median of the 3 judges' rating. Each wine sample has data detailing its chemical and physical properties like pH (measure of acidity), the amount of certain chemical compounds, the density and the total amount of alcohol. \n",
    "\n",
    "Some immediate questions concerning this dataset:\n",
    "\n",
    "    - Can we CLASSIFY samples as red or white wine with high accuracy given the sample data?\n",
    "    - Can we CLASSIFY samples as high or low quality with high accuracy given the sample data?\n",
    "    - Can we PREDICT the numeric quality of a sample with high accuracy (ie. replace tasters with computers)?\n",
    "    - How consistent and/or accurate are wine-tasting experts at identifying an objectively \"good\" sample?\n",
    "    \n",
    "Before we start, it is certainly worth asking the question: is there an objective set of chemical and physical properties that makes a wine good or bad? There are different _types_ of wines of the same colour, each of which are differentiated by things like the type of grapes used, added fruits or flavourings, aging times and temperatures, manner of storage and varying nutrient densities in the soils of the vineyard. Rephrasing the question, _for a given type of wine_, is there an objective set of properties that makes it good or bad? Without too much knowledge of how wine-tasters taste, I'd assume that tasters are indeed looking for particular qualities that they will identify with their tastebuds and no knowledge of the actual chemical structure of the wine. For example, a chef knows what a filet mignon _should_ taste like given the way it has historically been cooked, and thus knows what to look for in the taste of a \"good\" filet mignon.  \n",
    "\n",
    "Interestingly, there exist many studies that show wine-tasting experts often \"incorrectly\" rate the quality of wine in international competitions. By incorrectly, we mean rating wines that have won awards for quality in other competitions as poor, or vice-versa (Hodgson 2012, Journal of Wine Economics). These competitions are often blind taste tests with no information about the tasted wine (no bottle shape, no label or mention of accrued awards, no suggested flavours or time of aging) and thus a rating depends exclusively on _taste_. This does not necessarily imply that there are no objective qualities of good or bad wines, but that the sensory experience of taste itself is heavily influenced by other factors like the label design and colour, knowledge of awards, bottle shape, wine texture, etc. Those involved in the food industry are well aware that presentation is key - nobody wants to eat something visually unappetizing even if it tastes the same as a regular steak.\n",
    "\n",
    "Tasting experts can even be fooled into believing a _white wine_ is a red one if it is died red and _presented_ as a red wine (Morrot 2001, Brain and Language). This implies that, despite any findings we may produce that clearly differentiate red from white wine on a chemical or physical basis, an expert wine-taster may not be able to properly _interpret_ these differences by the tongue. Fascinating!\n",
    "    \n",
    "**VARIABLE DESCRIPTION:**\n",
    "    - QUALITY (integer in [1 - 10])\n",
    "    - FIXED ACIDITY (numeric, g/dm$^3$)\n",
    "    - VOLATILE ACIDITY (numeric, g/dm$^3$)\n",
    "    - CITRIC ACID (numeric, g/dm$^3$)\n",
    "    - RESIDUAL SUGAR (numeric, g/dm$^3$)\n",
    "    - CHLORIDES (numeric, g/dm$^3$)\n",
    "    - FREE SULFUR DIOXIDE (numeric, mg/dm$^3$)\n",
    "    - TOTAL SULFUR DIOXIDE (numeric, mg/dm$^3$)\n",
    "    - DENSITY (numeric, g/dm$^3$)\n",
    "    - pH (numeric, <= 14)\n",
    "    - SULPHATES (numeric, g/dm$^3$)\n",
    "    - ALCOHOL (numeric, [0.0 - 1.0], % volume)\n",
    "    \n",
    "Units for most quantities are in MASS DENSITY, with grams (g) or milligrams (mg) per cubic decimeter (dm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "def PlottingDefault():\n",
    "    sns.set(rc={'figure.figsize':(11.7,8.27),\"font.size\":25,\"axes.titlesize\":25,\"axes.labelsize\":25},style=\"white\")\n",
    "PlottingDefault()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data and describe it.\n",
    "df = pd.read_csv('/Users/reubengazer/Downloads/winequalityN.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with observations missing values. We can interpolate/impute values or remove observations altogether.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = df[df.isnull().values == True].shape[0]\n",
    "print \"There are {} rows ({}% of data) with at least one missing cell.\".format(n,round(n/float(df.shape[0]),2))\n",
    "# With such a small percentage, we can simply remove all \"error\" rows without worry.\n",
    "print \"Removing rows with missing values...\"\n",
    "df.drop(df[df.isnull().values == True].index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost exactly 3 times as many white wines as red in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 1a: Pairplot of All Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red wine is displayed as RED and white as GREEN (white markers would be difficult!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='type',hue_order=['red','white'],palette=['red','green'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appear to be obvious visual differences in physical and chemical properties between the red and white wine populations. As expected, these wines are inherently different enough to build an accurate classifier to predict the wine type. Let's also make a heatmap to see correlation numbers to begin feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 1b: Heatmap of All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10],dpi=2000)\n",
    "sns.heatmap(df.corr(),cmap='coolwarm',linewidth=2,annot=True,annot_kws={'fontsize':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption 1a, 1b\n",
    "\n",
    "From the above two correlation plots, some noteworthy correlations are:\n",
    "\n",
    "    - Density vs. Alcohol, Residual Sugar or Fixed Acidity (negative) \n",
    "        - alcohol is less dense than water, therefore higher alcohol = lower density, holding all else constant\n",
    "        - residual sugar is denser than water. Higher sugar = higher density, holding all else constant\n",
    "    - pH vs. any type of acid \n",
    "        -the pH is a directly dependent on the relative amounts of each acid, which each have their own pH\n",
    "    - Free and Total Sulfur Dioxide\n",
    "        - One comprises the other\n",
    "    - Quality vs. Alcohol (positive)\n",
    "    - Quality vs. Density (negative) \n",
    "        - but recall ALCOHOL and DENSITY are themselves correlated physically (negatively)\n",
    "    - Quality vs. Volatile Acidity (negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Red and White Wine Samples\n",
    "Let's leave all predictors in for the moment and build a logistic regresser that predicts the wine type with __sklearn__ to classify wine type.  \n",
    "First, build red and white copy data sets for syntax ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make copies for red and white datasets.\n",
    "red,white = df[df['type']=='red'],df[df['type']=='white']\n",
    "\n",
    "print(\"Dataset comprised of:\\n{:3.0f}% are RED\\n{:3.0f}% are WHITE\\n\".format(100*float(len(red))/len(df),100*float(len(white))/len(df)))\n",
    "\n",
    "# How many of each type of wine are, red and white?\n",
    "for wine in ['red','white']:\n",
    "    print \"Number of {} wines = {}\".format(wine,len(df[df['type']==wine]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: Boxplot Distributions of All Predictors (Split by Red, White)\n",
    "How different are the predictor distributions for each wine type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=6,figsize=[20,10])\n",
    "columngrid = np.array(df.columns[df.columns!='type']).reshape(2,6)\n",
    "\n",
    "for i,column in enumerate(df.columns[df.columns!='type']):\n",
    "    ind1,ind2 = np.where(columngrid==column)\n",
    "    ind1,ind2=ind1[0],ind2[0]\n",
    "    sns.boxplot(x='type',y=column,data=df,ax=axes[ind1,ind2],palette=['yellow','red'])\n",
    "    #axes[ind1,ind2].set_title(column+'\\n',fontsize=25)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grid displays the \"fingerprint\" of both red and white wine samples. . Visually it appears red and white wines differ in most categories and are only similar in the quality ratings and the alcohol content. \n",
    "\n",
    "Red wines are characterized apart from white wines by:\n",
    "    - Larger FIXED ACIDITY, VOLATILE ACIDITY, CHLORIDES, pH, SULPHATES, DENSITY\n",
    "    - Lower CITRIC ACID, RESIDUAL SUGAR, FREE and TOTAL SULFUR DIOXIDE\n",
    "\n",
    "Notably, _the quality of the wine is independent of the type of wine_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Student's T-test for Significant Differences of Mean Predictor Values in Red, White Wine**  \n",
    "Group by colour/type and show the mean of each aggregate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby('type').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize dataframe for t, p statistics outputs with index as each attribute save type.\n",
    "tdf=pd.DataFrame(index=[colname for colname in df.columns[df.columns!='type']])\n",
    "t,p,sig=[],[],[]\n",
    "for colname in df.columns[df.columns!='type']: # type is a string and we've grouped by it.\n",
    "    alpha = 0.05 # significance tolerance.\n",
    "    t2, p2 = stats.ttest_ind(red[colname],white[colname],equal_var=False) # Welch's 2-sided t-test.\n",
    "    t.append(t2)\n",
    "    p.append(p2)\n",
    "    if p2<alpha:\n",
    "        sig.append(True)\n",
    "    else:\n",
    "        sig.append(False)\n",
    "# Assign columns.\n",
    "tdf['t'],tdf['p'],tdf['Significant?'] = t,p,sig\n",
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although all differences in means between red and white are statistically significant, this does not mean that the differences are IMPORTANT to us. For example, the fractional differences between the means in QUALITY is incredibly small (2%), so this doesn't really reflect a large enough difference for us to care. However, mean values of predictors like total sulfur dioxide are not only statistically different by the t-test but almost importantly different for the fractional difference is quite large (~50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I: Logistic Classifier of Wine Colour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurately can we classify the type of wine given its predictors? Let's build a simple model that classifies the colour of the wine. Given the obvious visual differences in predictor distributions in Plot 2 by colour, the classes appear significantly recognizable and a highly accurate classifier seems plausible.\n",
    "\n",
    "Let's start with a logistic regression model including all predictors to get an initial look at classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize train and test sets of 80% and 20% of the data respectively.\n",
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_predict, cross_val_score\n",
    "X,y = df.drop('type',axis=1),df['type']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "# Train a logistic model on the training data.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "# Print the coefficients of the above model.\n",
    "print(\"\\nLogistic Regression Coefficients:\\n\")\n",
    "print(pd.DataFrame.from_records(zip(df.columns[df.columns!='type'],logmodel.coef_[0]),columns=['Predictor','Coef']))\n",
    "\n",
    "# Estimate the test error of the model using 5-Fold Cross Validation.\n",
    "k = 5\n",
    "cv = KFold(len(X_train),n_folds=k,shuffle=True)\n",
    "log_accuracy = cross_val_score(logmodel, X_train, y_train, cv=cv).mean()\n",
    "print \"\\nModel Accuracy With {}-Fold CV = {:.2f}%\".format(k,100*log_accuracy)\n",
    "\n",
    "# Make predictions with our logistic model, and produce some quality metrics.\n",
    "predictions = pd.Series(cross_val_predict(logmodel,X=X_test,y=y_test))\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "fig,ax=plt.subplots(1)\n",
    "sns.heatmap(confusion_matrix(y_test,predictions),annot=True,fmt='g',cbar_kws={'cmap':'coolwarm'},ax=ax)\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.xaxis.set_ticklabels(['red', 'white'],fontsize=20); ax.yaxis.set_ticklabels(['red', 'white'],fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it was fairly easy to predict a red or white wine given the types of predictors available. Without much thought as to feature selection and including all variables, we created a classifier with only a ~2% error rate. The worst we could do is to guess a constant classifier of white wine which produces an error rate of 9% (red comprises 9% of the dataset). Our fairly naive classifier is already \\better by ~7% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Model II: Classification of Wine Quality \n",
    "Let's classify the data by separating out wines with quality ratings of:\n",
    "\n",
    "    - [8-10] : 'good' \n",
    "    - [4-7]  : 'mid'\n",
    "    - [0-3]  : 'bad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many are good or bad, and are they predominantly red or white?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['Quality Class'] = 'mid'\n",
    "df.loc[df['quality']>=8,'Quality Class'] = 'good'\n",
    "df.loc[df['quality']<=3,'Quality Class'] = 'bad'\n",
    "\n",
    "good, bad = df[df['quality']>=8],df[df['quality']<=3]\n",
    "\n",
    "print(\"Dataset comprised of:\\n{:3.0f}% are RED\\n{:3.0f}% are WHITE\\n\".format(100*float(len(red))/len(df),100*float(len(white))/len(df)))\n",
    "print(\"Of the {} GOOD wines:\\n{:4.0f}% are RED\\n{:4.0f}% are WHITE\\n\".format(len(good),100*float(len(good[good['type']=='red']))/len(good),100*float(len(good[good['type']!='red']))/len(good)))\n",
    "print(\"Of the {} BAD wines:\\n{:4.0f}% are RED\\n{:4.0f}% are WHITE\".format(len(bad),100*float(len(bad[bad['type']=='red']))/len(bad),100*float(len(bad[bad['type']!='red']))/len(bad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists a larger fraction of GOOD wines that are WHITE than the actual population fraction of whites (75%), and a larger fraction of BAD wines that are RED than the actual fraction of reds (25%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 3a: Quality Class vs. All Predictors  \n",
    "How different are the predictor distributions for each wine type? Each of these plots will show any trends that exist between bad and good wines in general, independent of the wine colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=6,figsize=[20,10])\n",
    "columngrid = np.array(df.columns.drop(['type','Quality Class'])).reshape(2,6)\n",
    "\n",
    "for i,column in enumerate(df.columns.drop(['type','Quality Class','quality'])):\n",
    "    ind1,ind2 = np.where(columngrid==column)\n",
    "    ind1,ind2=ind1[0],ind2[0]\n",
    "    sns.boxplot(x='Quality Class',y=column,data=df,ax=axes[ind1,ind2],order=['bad','mid','good'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots are of Quality Class vs. Predictor trends for both types, but there are ~3x more white wine samples as red. Therefore these trends are heavily weighted towards the trends of white wine, and we should stratify this plot by colour for a better interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 3b: Quality Class vs. All Predictors, Stratified by Colour/Type\n",
    "Repeat the same as above, except stratify by the colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=6,figsize=[20,10])\n",
    "columngrid = np.array(df.columns.drop(['type','Quality Class'])).reshape(2,6)\n",
    "\n",
    "for i,column in enumerate(df.columns.drop(['type','Quality Class','quality'])):\n",
    "    ind1,ind2 = np.where(columngrid==column)\n",
    "    ind1,ind2=ind1[0],ind2[0]\n",
    "    sns.boxplot(x='Quality Class',y=column,data=df,ax=axes[ind1,ind2],order=['bad','mid','good'],hue='type',palette=['yellow','red'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that colour plays a key role in trend interpretation. Sulphates vs. quality class in 3a indicates a non-trend across colour. Plot 3b shows the red samples trend positively with amount of sulphates, but this is masked in 3a due to red presence in the data of only 25%. This motivates us to split the data by colour permanently and describe wine sample properties specific to their colour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind that each trend is interpreted _with all other variables held constant_, the takeaways from each of the above plots, in order:\n",
    "\n",
    "    - FIXED ACIDITY is not a strong predictor of quality\n",
    "    - Red wine quality is higher with less VOLATILE ACIDITY, CHLORIDES, more CITRIC ACID, SULPHATES\n",
    "    - White wine is not significantly affected by VOLATILE ACIDITY, CITRIC ACID, CHLORIDES, SULPHATES\n",
    "    - RESIDUAL SUGAR has no impact on quality rating across both colours\n",
    "    - Red wine is better when more acidic (lower pH), white wine quality is independent of pH\n",
    "    - The BEST RED AND/OR WHITE WINES HAVE THE HIGHEST ALCOHOL CONTENT (~2% higher than wines rated below 8)\n",
    "    \n",
    "And remember our conclusions about properties of red and white wines in general:\n",
    "\n",
    "    - REDS have larger FIXED ACIDITY, VOLATILE ACIDITY, CHLORIDES, pH, SULPHATES, DENSITY than WHITES\n",
    "    - REDS have lower CITRIC ACID, RESIDUAL SUGAR, FREE and TOTAL SULFUR DIOXIDE than WHITES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression for Classification of Wine Quality**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold, cross_val_predict, cross_val_score\n",
    "\n",
    "# Convert wine type to dummies.\n",
    "dums = pd.get_dummies(df['type'],drop_first=True)\n",
    "df = pd.concat([df,dums],axis=1)\n",
    "good_or_bad = df[df['Quality Class']!='mid'].copy()\n",
    "\n",
    "# Split into training and testing sets.\n",
    "X,y = good_or_bad.drop(['type','quality','Quality Class'],axis=1),good_or_bad['Quality Class']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "# Train logistic model on training set.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "\n",
    "# Print the coefficients of the model fitted on the training set.\n",
    "print(\"\\nLogistic Regression Coefficients:\\n\")\n",
    "print(pd.DataFrame.from_records(zip(X.columns,logmodel.coef_[0]),columns=['Predictor','Coef']))\n",
    "\n",
    "# Estimate the test-error using K-Fold Cross Validation.\n",
    "cv = KFold(len(X_train),n_folds=5,shuffle=True)\n",
    "log_accuracy = cross_val_score(logmodel, X_train, y_train, cv=cv).mean()\n",
    "print \"\\nModel Accuracy or Estimate Test Error (via 5-Fold CV) = {:.6f}%\".format(100*log_accuracy)\n",
    "\n",
    "# Classify the testing data X_test, y_test and see accuracy report.\n",
    "predictions = cross_val_predict(logmodel,X=X_test,y=y_test)\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test,predictions))\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "fig,ax=plt.subplots(1)\n",
    "sns.heatmap(confusion_matrix(y_test,predictions),annot=True)\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.xaxis.set_ticklabels(['good', 'bad'],fontsize=20); ax.yaxis.set_ticklabels(['good', 'bad'],fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale the predictors.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X,y = good_or_bad.drop(['type','quality','Quality Class'],axis=1),good_or_bad['Quality Class']\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "scaled_features = scaler.transform(X)\n",
    "X_feat = pd.DataFrame(scaled_features,columns=X.columns) # scaled X matrix of predictors.\n",
    "\n",
    "# Produce a final validation set.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_feat,y,test_size=0.2)\n",
    "\n",
    "# Track accuracy of classifier (using 10-fold CV) for each value of k in carray cv_scores.\n",
    "cv_scores = []\n",
    "for k in range(1,20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    scores = cross_val_score(knn,X_train,y_train,cv=10,scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "    \n",
    "# Compute optimal k, plot misclassification rate vs. k.\n",
    "MSE = [1-x for x in cv_scores] # misclassification rate\n",
    "optimal_k = range(1,20)[MSE.index(min(MSE))]\n",
    "print(\"The optimal value of k is {}.\".format(optimal_k))\n",
    "    \n",
    "# Plot error rate vs. choice of k-neighbours.\n",
    "fig, ax = plt.subplots(1,figsize=[6,6])\n",
    "ax.plot(range(1,20), MSE, color='blue', linestyle='--', marker='o', markerfacecolor='red', markersize=10)\n",
    "ax.set_xlabel('K-Neighbours')\n",
    "ax.set_ylabel('Error Rate')\n",
    "\n",
    "# With optimal k chosen, perform fit and evaluate on the test data.\n",
    "knn = KNeighborsClassifier(n_neighbors=optimal_k)\n",
    "knn_accuracy = cross_val_score(knn,X_test,y_test,cv=10,scoring='accuracy').mean()\n",
    "print \"\\nModel Accuracy or Estimate Test Error (via 10-Fold CV) = {:.6f}%\".format(100*knn_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
